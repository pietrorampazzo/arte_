#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
üöÄ RATE LIMITER OTIMIZADO - BATCHES INTELIGENTES
Vers√£o: Estado da Arte - Janeiro 2025

FUNCIONALIDADES:
‚úÖ Rate limiting inteligente para Google Gemini
‚úÖ Batches adaptativos baseados em tokens
‚úÖ Estimativa precisa de tokens
‚úÖ Recupera√ß√£o autom√°tica de erros
‚úÖ Logging detalhado para monitoramento

LIMITES GOOGLE GEMINI 2.0 FLASH-LITE:
- RPM: 30 solicita√ß√µes por minuto
- TPM: 1.000.000 tokens por minuto  
- RPD: 200 solicita√ß√µes por dia

ESTRAT√âGIA DE BATCHES:
- Batch padr√£o: 20 itens (~5.000 tokens)
- Batch adaptativo: Ajusta baseado no TPM dispon√≠vel
- Fallback: Reduz batch em caso de erro

Autor: Sistema Otimizado
"""

import time
import logging
from datetime import datetime, timedelta
import pandas as pd
import json
import re

# Configurar logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class RateLimiterOtimizado:
    """Rate limiter inteligente para APIs com batches adaptativos"""
    
    def __init__(self, rpm=30, tpm=1_000_000, rpd=200, batch_size_default=200):
        # Limites da API
        self.rpm = rpm
        self.tpm = tpm
        self.rpd = rpd
        
        # Configura√ß√£o de batches
        self.batch_size_default = batch_size_default
        self.batch_size_current = batch_size_default
        self.batch_size_min = 5
        self.batch_size_max = 50
        
        # Hist√≥rico de solicita√ß√µes
        self.requests_in_minute = []
        self.tokens_in_minute = []
        self.requests_in_day = []
        
        # Controle de tempo
        self.day_start = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)
        self.last_request_time = None
        
        # Estat√≠sticas
        self.total_requests = 0
        self.total_tokens = 0
        self.total_errors = 0
        
        logger.info("Rate Limiter inicializado: RPM=%d, TPM=%d, RPD=%d, Batch=%d", 
                   rpm, tpm, rpd, batch_size_default)
    
    def estimate_tokens_precise(self, texts):
        """
        Estimativa precisa de tokens baseada em an√°lise de texto
        
        M√âTODO MELHORADO:
        - Conta palavras, n√∫meros, pontua√ß√£o
        - Considera tokens especiais (JSON, formata√ß√£o)
        - Margem de seguran√ßa de 20%
        """
        if not texts:
            return 0
        
        total_tokens = 0
        
        for text in texts:
            if not text or pd.isna(text):
                continue
            
            text_str = str(text)
            
            # Contagem b√°sica: 1 token ‚âà 4 caracteres (conservador)
            base_tokens = len(text_str) // 3.5  # Mais preciso que //4
            
            # Ajustes para diferentes tipos de conte√∫do
            
            # Texto t√©cnico (mais tokens por palavra)
            if any(term in text_str.upper() for term in ['ESPECIFICA√á√ïES', 'T√âCNICO', 'INSTRUMENTO']):
                base_tokens *= 1.2
            
            # JSON/estruturado (tokens extras para formata√ß√£o)
            if '{' in text_str or '[' in text_str:
                base_tokens *= 1.3
            
            # N√∫meros e medidas (tokens extras)
            numbers = len(re.findall(r'\d+', text_str))
            base_tokens += numbers * 0.5
            
            # Pontua√ß√£o complexa
            punctuation = len(re.findall(r'[,;:.!?()[\]{}"]', text_str))
            base_tokens += punctuation * 0.2
            
            total_tokens += base_tokens
        
        # Margem de seguran√ßa de 20%
        total_tokens = int(total_tokens * 1.2)
        
        # M√≠nimo de 10 tokens por texto n√£o vazio
        total_tokens = max(total_tokens, len([t for t in texts if t and not pd.isna(t)]) * 10)
        
        return total_tokens
    
    def calculate_optimal_batch_size(self, available_tpm, avg_tokens_per_item=250):
        """Calcula tamanho √≥timo do batch baseado no TPM dispon√≠vel"""
        
        # Calcular quantos itens cabem no TPM dispon√≠vel
        max_items_by_tpm = available_tpm // avg_tokens_per_item
        
        # Limitar pelos constraints
        optimal_size = min(
            max_items_by_tpm,
            self.batch_size_max,
            max(self.batch_size_min, max_items_by_tpm)
        )
        
        return int(optimal_size)
    
    def update_batch_size_adaptive(self, success=True, tokens_used=0):
        """Atualiza tamanho do batch adaptativamente baseado no sucesso"""
        
        if success:
            # Sucesso: pode tentar aumentar o batch (gradualmente)
            if self.batch_size_current < self.batch_size_max:
                self.batch_size_current = min(
                    self.batch_size_current + 2,
                    self.batch_size_max
                )
                logger.debug("Batch size aumentado para: %d", self.batch_size_current)
        else:
            # Erro: reduzir batch para ser mais conservador
            self.batch_size_current = max(
                self.batch_size_current // 2,
                self.batch_size_min
            )
            logger.warning("Batch size reduzido para: %d devido a erro", self.batch_size_current)
    
    def get_current_limits_status(self):
        """Retorna status atual dos limites"""
        now = datetime.now()
        
        # Limpar hist√≥rico antigo
        self._clean_old_records(now)
        
        # Calcular uso atual
        current_rpm = len(self.requests_in_minute)
        current_tpm = sum(tokens for _, tokens in self.tokens_in_minute)
        current_rpd = len(self.requests_in_day)
        
        # Calcular disponibilidade
        available_rpm = self.rpm - current_rpm
        available_tpm = self.tpm - current_tpm
        available_rpd = self.rpd - current_rpd
        
        return {
            'rpm': {'current': current_rpm, 'limit': self.rpm, 'available': available_rpm},
            'tpm': {'current': current_tpm, 'limit': self.tpm, 'available': available_tpm},
            'rpd': {'current': current_rpd, 'limit': self.rpd, 'available': available_rpd},
            'batch_size': self.batch_size_current
        }
    
    def _clean_old_records(self, now):
        """Remove registros antigos do hist√≥rico"""
        
        # Atualizar janela de um dia
        if now >= self.day_start + timedelta(days=1):
            self.day_start = now.replace(hour=0, minute=0, second=0, microsecond=0)
            self.requests_in_day = []
            logger.info("Janela di√°ria reiniciada. RPD resetado.")
        
        # Limpar registros de mais de 1 minuto
        minute_ago = now - timedelta(minutes=1)
        self.requests_in_minute = [t for t in self.requests_in_minute if t > minute_ago]
        self.tokens_in_minute = [(t, tokens) for t, tokens in self.tokens_in_minute if t > minute_ago]
    
    def can_make_request(self, texts):
        """
        Verifica se pode fazer solicita√ß√£o com os textos fornecidos
        
        RETORNA:
        - True: Pode fazer a solicita√ß√£o
        - False: Deve aguardar
        - int: Segundos para aguardar
        """
        now = datetime.now()
        self._clean_old_records(now)
        
        # Estimar tokens necess√°rios
        tokens_needed = self.estimate_tokens_precise(texts)
        
        # Verificar limites
        status = self.get_current_limits_status()
        
        # Verificar RPD (limite mais restritivo)
        if status['rpd']['available'] <= 0:
            logger.error("Limite di√°rio atingido: %d/%d RPD", status['rpd']['current'], self.rpd)
            return False
        
        # Verificar RPM
        if status['rpm']['available'] <= 0:
            wait_time = 60 - (now - self.requests_in_minute[0]).total_seconds()
            logger.info("Limite RPM atingido. Aguardando %.1f segundos.", wait_time)
            return int(wait_time) + 1
        
        # Verificar TPM
        if tokens_needed > status['tpm']['available']:
            if self.tokens_in_minute:
                wait_time = 60 - (now - self.tokens_in_minute[0][0]).total_seconds()
                logger.info("Limite TPM atingido. Tokens necess√°rios: %d, dispon√≠veis: %d. Aguardando %.1f segundos.", 
                           tokens_needed, status['tpm']['available'], wait_time)
                return int(wait_time) + 1
            else:
                # Tokens necess√°rios excedem limite total
                logger.error("Batch muito grande: %d tokens > %d TPM", tokens_needed, self.tpm)
                return False
        
        return True
    
    def wait_if_needed(self, texts):
        """Aguarda se necess√°rio antes de fazer a solicita√ß√£o"""
        
        max_retries = 3
        retry_count = 0
        
        while retry_count < max_retries:
            can_proceed = self.can_make_request(texts)
            
            if can_proceed is True:
                return True
            elif can_proceed is False:
                logger.error("N√£o √© poss√≠vel fazer a solicita√ß√£o (limite di√°rio ou batch muito grande)")
                return False
            else:
                # can_proceed √© o tempo de espera
                wait_time = can_proceed
                logger.info("Aguardando %d segundos antes da pr√≥xima tentativa...", wait_time)
                time.sleep(wait_time)
                retry_count += 1
        
        logger.error("M√°ximo de tentativas atingido")
        return False
    
    def record_request(self, texts, success=True):
        """Registra uma solicita√ß√£o realizada"""
        now = datetime.now()
        tokens_used = self.estimate_tokens_precise(texts)
        
        if success:
            # Registrar solicita√ß√£o bem-sucedida
            self.requests_in_minute.append(now)
            self.tokens_in_minute.append((now, tokens_used))
            self.requests_in_day.append(now)
            
            # Atualizar estat√≠sticas
            self.total_requests += 1
            self.total_tokens += tokens_used
            
            # Atualizar batch size adaptativamente
            self.update_batch_size_adaptive(success=True, tokens_used=tokens_used)
            
            logger.debug("Solicita√ß√£o registrada: %d tokens, RPM: %d/%d, TPM: %d/%d, RPD: %d/%d", 
                        tokens_used, len(self.requests_in_minute), self.rpm,
                        sum(t for _, t in self.tokens_in_minute), self.tpm,
                        len(self.requests_in_day), self.rpd)
        else:
            # Registrar erro
            self.total_errors += 1
            self.update_batch_size_adaptive(success=False)
            logger.warning("Erro registrado. Total de erros: %d", self.total_errors)
        
        self.last_request_time = now
    
    def get_statistics(self):
        """Retorna estat√≠sticas de uso"""
        status = self.get_current_limits_status()
        
        return {
            'total_requests': self.total_requests,
            'total_tokens': self.total_tokens,
            'total_errors': self.total_errors,
            'current_status': status,
            'batch_size_current': self.batch_size_current,
            'success_rate': (self.total_requests / max(self.total_requests + self.total_errors, 1)) * 100
        }
    
    def suggest_batch_size(self, total_items, avg_tokens_per_item=250):
        """Sugere tamanho de batch √≥timo para processar todos os itens"""
        status = self.get_current_limits_status()
        
        # Calcular baseado no TPM dispon√≠vel
        optimal_by_tpm = self.calculate_optimal_batch_size(
            status['tpm']['available'], avg_tokens_per_item
        )
        
        # Calcular baseado no RPD dispon√≠vel
        remaining_requests = status['rpd']['available']
        optimal_by_rpd = total_items // max(remaining_requests, 1)
        
        # Usar o mais conservador
        suggested_size = min(optimal_by_tpm, optimal_by_rpd, self.batch_size_max)
        suggested_size = max(suggested_size, self.batch_size_min)
        
        logger.info("Batch sugerido: %d (TPM: %d, RPD: %d, atual: %d)", 
                   suggested_size, optimal_by_tpm, optimal_by_rpd, self.batch_size_current)
        
        return int(suggested_size)

# Fun√ß√£o utilit√°ria para importa√ß√£o
def create_rate_limiter():
    """Cria inst√¢ncia do rate limiter com configura√ß√µes padr√£o"""
    return RateLimiterOtimizado()

# Teste b√°sico
if __name__ == "__main__":
    import pandas as pd
    
    print("üöÄ Testando Rate Limiter Otimizado")
    
    # Criar rate limiter
    limiter = RateLimiterOtimizado()
    
    # Simular textos
    textos_teste = [
        "Trompete em Bb com 3 pistos, acabamento dourado",
        "Caixa de som ativa 15 polegadas 400W RMS",
        "Baqueta de madeira para bateria, ponta de nylon"
    ]
    
    # Testar estimativa de tokens
    tokens = limiter.estimate_tokens_precise(textos_teste)
    print(f"Tokens estimados: {tokens}")
    
    # Testar verifica√ß√£o de limites
    can_proceed = limiter.can_make_request(textos_teste)
    print(f"Pode prosseguir: {can_proceed}")
    
    # Simular solicita√ß√£o
    if can_proceed is True:
        limiter.record_request(textos_teste, success=True)
        print("Solicita√ß√£o registrada com sucesso")
    
    # Mostrar estat√≠sticas
    stats = limiter.get_statistics()
    print(f"Estat√≠sticas: {json.dumps(stats, indent=2, default=str)}")
    
    print("‚úÖ Teste conclu√≠do")


#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
üß† GERADOR DE EMBEDDINGS SIMPLES - FUNCIONAL
Vers√£o: Simplificada - Janeiro 2025

OBJETIVO: Gerar embeddings dos textos processados
‚úÖ L√™ textos do processador Groq
‚úÖ Gera embeddings com Sentence Transformers
‚úÖ Cria √≠ndice FAISS
‚úÖ Salva mapeamento completo
‚úÖ Pronto para busca

FOCO: SIMPLICIDADE E FUNCIONALIDADE
Complementa o processador_simples_groq.py

Autor: Sistema Simplificado
"""

import pandas as pd
import numpy as np
import faiss
import os
import logging
from sentence_transformers import SentenceTransformer
from tqdm import tqdm
import pickle

# Configurar logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# === CONFIGURA√á√ïES ===
PASTA_DADOS = "dados_processados"
PASTA_INDICE = "indice_simples"

# Arquivos de entrada (gerados pelo processador)
ARQUIVO_PRODUTOS = os.path.join(PASTA_DADOS, "produtos_processados.xlsx")
ARQUIVO_TEXTOS = os.path.join(PASTA_DADOS, "textos_para_embedding.txt")

# Arquivos de sa√≠da
ARQUIVO_INDICE_FAISS = os.path.join(PASTA_INDICE, "produtos.index")
ARQUIVO_MAPEAMENTO = os.path.join(PASTA_INDICE, "mapeamento.xlsx")
ARQUIVO_EMBEDDINGS = os.path.join(PASTA_INDICE, "embeddings.npy")

# Modelo de embeddings
MODELO_EMBEDDING = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
BATCH_SIZE_EMBEDDING = 32

class GeradorEmbeddingsSimples:
    """Gerador de embeddings super simples que funciona"""
    
    def __init__(self):
        logger.info("üß† Inicializando gerador de embeddings...")
        
        try:
            self.model = SentenceTransformer(MODELO_EMBEDDING)
            logger.info(f"‚úÖ Modelo carregado: {MODELO_EMBEDDING}")
        except Exception as e:
            logger.error(f"‚ùå Erro ao carregar modelo: {e}")
            raise
        
        # Criar pasta de √≠ndice
        os.makedirs(PASTA_INDICE, exist_ok=True)
    
    def carregar_dados(self):
        """Carrega dados processados"""
        
        logger.info("üìÇ Carregando dados processados...")
        
        # Verificar se arquivos existem
        if not os.path.exists(ARQUIVO_PRODUTOS):
            raise FileNotFoundError(f"Arquivo n√£o encontrado: {ARQUIVO_PRODUTOS}")
        
        if not os.path.exists(ARQUIVO_TEXTOS):
            raise FileNotFoundError(f"Arquivo n√£o encontrado: {ARQUIVO_TEXTOS}")
        
        # Carregar DataFrame
        self.df_produtos = pd.read_excel(ARQUIVO_PRODUTOS)
        logger.info(f"‚úÖ DataFrame carregado: {len(self.df_produtos)} produtos")
        
        # Carregar textos para embedding
        with open(ARQUIVO_TEXTOS, 'r', encoding='utf-8') as f:
            self.textos_embedding = [linha.strip() for linha in f.readlines()]
        
        logger.info(f"‚úÖ Textos carregados: {len(self.textos_embedding)} textos")
        
        # Verificar consist√™ncia
        if len(self.df_produtos) != len(self.textos_embedding):
            logger.warning(f"‚ö†Ô∏è Inconsist√™ncia: {len(self.df_produtos)} produtos vs {len(self.textos_embedding)} textos")
            # Ajustar para o menor
            min_len = min(len(self.df_produtos), len(self.textos_embedding))
            self.df_produtos = self.df_produtos.iloc[:min_len]
            self.textos_embedding = self.textos_embedding[:min_len]
            logger.info(f"‚úÖ Ajustado para: {min_len} itens")
    
    def gerar_embeddings(self):
        """Gera embeddings dos textos"""
        
        logger.info("üß† Gerando embeddings...")
        
        # Filtrar textos vazios
        textos_validos = []
        indices_validos = []
        
        for i, texto in enumerate(self.textos_embedding):
            if texto and texto.strip():
                textos_validos.append(texto.strip())
                indices_validos.append(i)
            else:
                logger.warning(f"‚ö†Ô∏è Texto vazio no √≠ndice {i}")
        
        logger.info(f"üìù Textos v√°lidos: {len(textos_validos)}")
        
        # Gerar embeddings em batches
        embeddings_list = []
        
        for i in tqdm(range(0, len(textos_validos), BATCH_SIZE_EMBEDDING), desc="Gerando embeddings"):
            batch_textos = textos_validos[i:i + BATCH_SIZE_EMBEDDING]
            
            try:
                batch_embeddings = self.model.encode(
                    batch_textos,
                    batch_size=BATCH_SIZE_EMBEDDING,
                    show_progress_bar=False,
                    convert_to_numpy=True
                )
                embeddings_list.append(batch_embeddings)
                
            except Exception as e:
                logger.error(f"‚ùå Erro ao gerar embeddings do batch {i}: {e}")
                # Criar embeddings zeros como fallback
                fallback_embeddings = np.zeros((len(batch_textos), self.model.get_sentence_embedding_dimension()))
                embeddings_list.append(fallback_embeddings)
        
        # Concatenar todos os embeddings
        if embeddings_list:
            self.embeddings = np.vstack(embeddings_list).astype('float32')
            logger.info(f"‚úÖ Embeddings gerados: {self.embeddings.shape}")
        else:
            raise ValueError("Nenhum embedding foi gerado")
        
        # Ajustar DataFrame para corresponder aos embeddings v√°lidos
        self.df_produtos = self.df_produtos.iloc[indices_validos].reset_index(drop=True)
        self.textos_embedding = [self.textos_embedding[i] for i in indices_validos]
        
        logger.info(f"‚úÖ Dados ajustados: {len(self.df_produtos)} produtos finais")
    
    def criar_indice_faiss(self):
        """Cria √≠ndice FAISS"""
        
        logger.info("üîç Criando √≠ndice FAISS...")
        
        # Normalizar embeddings para similaridade de cosseno
        faiss.normalize_L2(self.embeddings)
        
        # Criar √≠ndice
        dimensao = self.embeddings.shape[1]
        self.index = faiss.IndexFlatIP(dimensao)  # Inner Product (cosseno ap√≥s normaliza√ß√£o)
        
        # Adicionar embeddings com IDs
        ids = np.arange(len(self.embeddings)).astype('int64')
        self.index = faiss.IndexIDMap(self.index)
        self.index.add_with_ids(self.embeddings, ids)
        
        logger.info(f"‚úÖ √çndice FAISS criado: {self.index.ntotal} vetores, dimens√£o {dimensao}")
    
    def salvar_resultados(self):
        """Salva todos os resultados"""
        
        logger.info("üíæ Salvando resultados...")
        
        # Salvar √≠ndice FAISS
        faiss.write_index(self.index, ARQUIVO_INDICE_FAISS)
        logger.info(f"‚úÖ √çndice FAISS salvo: {ARQUIVO_INDICE_FAISS}")
        
        # Salvar embeddings como numpy array
        np.save(ARQUIVO_EMBEDDINGS, self.embeddings)
        logger.info(f"‚úÖ Embeddings salvos: {ARQUIVO_EMBEDDINGS}")
        
        # Preparar mapeamento completo
        df_mapeamento = self.df_produtos.copy()
        df_mapeamento['texto_embedding'] = self.textos_embedding
        df_mapeamento['embedding_id'] = range(len(df_mapeamento))
        
        # Salvar mapeamento
        df_mapeamento.to_excel(ARQUIVO_MAPEAMENTO, index=False)
        logger.info(f"‚úÖ Mapeamento salvo: {ARQUIVO_MAPEAMENTO}")
        
        return {
            'total_produtos': len(df_mapeamento),
            'dimensao_embeddings': self.embeddings.shape[1],
            'arquivo_indice': ARQUIVO_INDICE_FAISS,
            'arquivo_mapeamento': ARQUIVO_MAPEAMENTO,
            'arquivo_embeddings': ARQUIVO_EMBEDDINGS
        }
    
    def testar_busca(self, texto_teste="trompete em Bb"):
        """Testa busca no √≠ndice criado"""
        
        logger.info(f"üß™ Testando busca com: '{texto_teste}'")
        
        try:
            # Gerar embedding da consulta
            embedding_consulta = self.model.encode([texto_teste], convert_to_numpy=True)
            embedding_consulta = embedding_consulta.astype('float32')
            faiss.normalize_L2(embedding_consulta)
            
            # Buscar
            k = 5
            distancias, indices = self.index.search(embedding_consulta, k)
            
            logger.info("üîç Resultados da busca:")
            for i, (idx, dist) in enumerate(zip(indices[0], distancias[0])):
                if idx != -1:
                    produto = self.df_produtos.iloc[idx]
                    logger.info(f"   {i+1}. {produto.get('Marca', 'N/A')} {produto.get('Modelo', 'N/A')} (similaridade: {dist:.3f})")
            
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Erro no teste de busca: {e}")
            return False

def main():
    """Fun√ß√£o principal"""
    
    print("üß† GERADOR DE EMBEDDINGS SIMPLES - VERS√ÉO FUNCIONAL")
    print("=" * 80)
    print("üéØ Objetivo: Gerar embeddings dos dados processados pelo Groq")
    print("üöÄ Foco: FUNCIONALIDADE e SIMPLICIDADE")
    print("=" * 80)
    
    try:
        # Verificar se dados processados existem
        if not os.path.exists(PASTA_DADOS):
            logger.error(f"‚ùå Pasta n√£o encontrada: {PASTA_DADOS}")
            logger.error("Execute primeiro: python processador_simples_groq.py")
            return
        
        # Criar gerador
        gerador = GeradorEmbeddingsSimples()
        
        # 1. Carregar dados
        gerador.carregar_dados()
        
        # 2. Gerar embeddings
        gerador.gerar_embeddings()
        
        # 3. Criar √≠ndice FAISS
        gerador.criar_indice_faiss()
        
        # 4. Salvar resultados
        resultados = gerador.salvar_resultados()
        
        # 5. Testar busca
        sucesso_teste = gerador.testar_busca()
        
        # 6. Estat√≠sticas finais
        print("\nüéâ GERA√á√ÉO DE EMBEDDINGS CONCLU√çDA!")
        print("=" * 80)
        print(f"üìä ESTAT√çSTICAS:")
        print(f"   Total de produtos: {resultados['total_produtos']}")
        print(f"   Dimens√£o embeddings: {resultados['dimensao_embeddings']}")
        print(f"   Teste de busca: {'‚úÖ Sucesso' if sucesso_teste else '‚ùå Falhou'}")
        print(f"\nüìÅ ARQUIVOS GERADOS:")
        print(f"   √çndice FAISS: {resultados['arquivo_indice']}")
        print(f"   Mapeamento: {resultados['arquivo_mapeamento']}")
        print(f"   Embeddings: {resultados['arquivo_embeddings']}")
        print("=" * 80)
        print("üöÄ PR√ìXIMO PASSO: Use o buscador para fazer consultas!")
        print("=" * 80)
        
    except Exception as e:
        logger.error(f"‚ùå Erro durante gera√ß√£o: {e}")
        print(f"\n‚ùå ERRO: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()

